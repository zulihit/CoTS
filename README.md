<div align="center">
  <h1> CVPR 2025 â€¢ Collaborative Tree Search for Enhancing Embodied Multi-Agent Collaboration </h1>
</div>



<p align="center">
<a href="https://arxiv.org/abs/XXXX.XXXXX" alt="arXiv">
    <img src="https://img.shields.io/badge/paper-Coming--soon-b31b1b.svg?style=flat" /></a>
<a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-1.x%20%7C%202.x-673ab7.svg" alt="Tested PyTorch Versions"></a>
<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/badge/License-MIT-4caf50.svg" alt="License"></a>
</p>

<p align="center">

<img src="assets/cots.png" width="550">
</p>

---

This repository contains the official implementation of our CVPR 2025 paper:

**ğŸ¤–Collaborative Tree Search for Enhancing Embodied Multi-Agent CollaborationğŸ¤–**  
_Lizheng Zu, Lin Lin, Song Fu, Na Zhao, Pan Zhou_  

> CoTS (Cooperative Tree Search) is a collaborative framework for embodied agents based on large language models, which enhances multi-agent planning and execution by guiding strategic discussions within a modified Monte Carlo Tree Search and evaluating plans to ensure coherent, efficient teamwork in complex tasks.

---
## ğŸ§  Framework of CoTS 

> **ğŸ§­ Plan â†’ ğŸª Reflect & Score â†’ ğŸŒ² Search â†’ âœ… Act**

CoTS builds upon the architecture of [CoELA](https://umass-embodied-agi.github.io/CoELA/), enables multiple LLM-based embodied agents to collaborate effectively by integrating large language models into a dynamic tree-based decision-making process. The four key stages are:

- ğŸ§­ **Plan**: Agents collaboratively propose high-level strategies using LLM-generated dialogues.
- ğŸª **Reflect & Score**: Plans are reflected upon using custom LLM-based reward signals.
- ğŸŒ² **Search**: A Monte Carlo Tree is built with branching proposals, allowing agents to evaluate multiple paths and correct one another.
- âœ… **Act**: Once a coherent plan is validated, agents execute actions in coordination. A plan evaluation module ensures consistency and adapts if plans become unsuitable.

<p align="center">
  <img src="assets/framework.png" width="700" alt="CoTS Framework Overview">
</p>

---

## ğŸŒ² Cooperative Tree Search

CoTS enables agents to collaborate within a shared Monte Carlo Tree by generating, evaluating, and optimizing plans through language. This collaborative tree search is built upon the [LangGraph](https://github.com/langchain-ai/langgraph). 

<p align="center">
  <img src="assets/tree_search.png" width="700" alt="Cooperative Tree Search">
</p>

> **ğŸ§© How it works:**  
> Each node in the tree contains content generated by **Alice** and **Bob**:  
> - ğŸ¤– *Alice* proposes collaborative plans and sends messages to Bob.  
> - ğŸ¤– *Bob* responds with messages to Alice and determines the plan rewards.  
> These rewards are used to backpropagate values, guiding the search process toward more promising and coherent joint plans.


---

## ğŸ“„ Paper & Resources

- **Paper (CVPR 2025)**: [Coming Soon] 
- **Project Website**: [Coming Soon]  
- ğŸ–¼**Poster**: [View Poster](https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/32902.png?t=1747986549.597608)

---


## Installation

For detailed instructions on the installation of the two embodied multi-agent environments `Communicative Watch-And-Help` and `ThreeDWorld Multi-Agent Transport`, please refer to the Setup sections in [`tdw_mat/README.md`](tdw_mat/README.md) and [`cwah/README.md`](cwah/README.md) respectively. 

---

## ğŸš€ Running CoTS

We provide ready-to-use scripts for running CoTS in the following multi-agent embodied environments:

- ğŸ—ï¸ **TDW-MAT** (`ThreeDWorld Multi-Agent Transport`) â€” [`tdw_mat/scripts`](tdw_mat/scripts)  
- ğŸ¤ **CWAH** (`Communicative Watch-And-Help`) â€” [`cwah/scripts`](cwah/scripts) 

â–¶ï¸ Example: Run CoTS in TDW-MAT using GPT-4

```bash
./scripts/test_LMs-gpt-4.sh
```
---

## ğŸ”‘ Notes on API Usage

This project uses the API in two parts. They occur in `tdw_mat/LLM/LLM.py` and `cwah/LLM/LLM.py`.

The first part is initialized as `client = OpenAI(model="", api_key="", base_url="")`. This part handles general natural language processing for the agents â€” such as interpreting observations, generating responses, or selecting actions â€” outside of the cooperative tree search process.

The second part is initialized as `mcts = MonteCarloTreeSearch(model="", api_key="", base_url="")`. This separation exists because collaborative planning often requires more advanced reasoning capabilities, and thus may benefit from stronger language models. It also allows for more flexibility in specifying different models for different components.

ğŸ’¡ **Important Note:** In our current paper and all reported experiments, we used the *same language model* (e.g., GPT-4) for both the general agent modules and the cooperative tree search. The separation of API calls is intended to support future exploration and modular improvements.

---

## ğŸ“š Citation
If you find CoTS helpful in your research, please consider citing:

```bibtex
@inproceedings{zu2025cots,
  title={Collaborative Tree Search for Enhancing Embodied Multi-Agent Collaboration},
  author={Zu, Lizheng and Lin, Lin and Fu, Song and Zhao, Na and Zhou, Pan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}
```

---

## ğŸ¤ Acknowledgements
Special thanks to the developers of CoELA and LangGraphâ€”your open-source efforts made CoTS possible! ğŸ™

---
